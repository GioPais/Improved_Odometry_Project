{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9BLjcn55NG8G"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bG43m3FsT583"
   },
   "outputs": [],
   "source": [
    "class SMnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SMnet, self).__init__()\n",
    "        self.linear_start1 = nn.Linear(20,128)#20,128\n",
    "        self.linear_start2 = nn.Linear(128,256)#128,256\n",
    "        #self.linear_start1.cuda()\n",
    "        #self.linear_start2.cuda()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256, \n",
    "            hidden_size=512, \n",
    "            num_layers=3,\n",
    "            batch_first=True)#256,512,3\n",
    "        \n",
    "       # self.lstm.cuda()\n",
    "        self.linear1 = nn.Linear(512, 256)#512,256\n",
    "        self.linear2 = nn.Linear(256, 3)#256,3\n",
    "        #self.linear1.cuda()\n",
    "        #self.linear2.cuda()\n",
    "        \n",
    "    def forward(self, robot_sensors):\n",
    "        ls_out1 = self.linear_start1(robot_sensors)\n",
    "        ls_out2 = self.linear_start2(ls_out1)\n",
    "        lstm_out, (h_n, h_c) = self.lstm(ls_out2)\n",
    "        l_out1 = self.linear1(lstm_out)\n",
    "        l_out2 = self.linear2(l_out1)\n",
    "\n",
    "        return l_out2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "Jau0NItlfoEM",
    "outputId": "13607ef7-92d0-4984-ca16-77ab71e66588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46874 datos de entrada con tamaño 21\n",
      "46874 datos de salida con tamaño  8\n",
      "46874 datos de entrada con tamaño 20\n",
      "46873 datos de salida con tamaño  3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "data_x= pd.read_csv('joints_tobar.txt',delimiter=\" \",header=None)\n",
    "data_y= pd.read_csv('ground_truth.txt',delimiter=\" \",header=None)\n",
    "feat=data_x.as_matrix()\n",
    "gtruth=data_y.as_matrix()\n",
    "\n",
    "print('{0:2d} datos de entrada con tamaño {1:2d}'.format(len(feat),len(feat[0])))\n",
    "print('{0:2d} datos de salida con tamaño {1:2d}'.format(len(gtruth),len(gtruth[0])))\n",
    "y=[]\n",
    "x=[]\n",
    "v=[]\n",
    "time=[]\n",
    "#print(ground_t)\n",
    "#Extraer x,y,z\n",
    "for i in range(0,len(feat)):\n",
    "    xi=[]\n",
    "    for j in range(1,len(feat[0])):\n",
    "        xi.append(feat[i][j])\n",
    "    x.append(xi)\n",
    "    time.append(feat[i][0])\n",
    "    #if i%10000==0:\n",
    "        #print(xi)\n",
    "        \n",
    "for i in range(0,len(gtruth)):\n",
    "    pos=[]\n",
    "    pos.append(gtruth[i][1])\n",
    "    pos.append(gtruth[i][2])\n",
    "    pos.append(gtruth[i][3])\n",
    "    y.append(pos)\n",
    "    #if i%10000==0:\n",
    "        #print(pos)\n",
    "\n",
    "for i in range(0,len(gtruth)-1):\n",
    "    vel=[]\n",
    "    vel.append(gtruth[i+1][1]-gtruth[i][1])\n",
    "    vel.append(gtruth[i+1][2]-gtruth[i][2])\n",
    "    vel.append(gtruth[i+1][3]-gtruth[i][3])\n",
    "    v.append(pos)\n",
    "\n",
    "y=v\n",
    "        \n",
    "print('{0:2d} datos de entrada con tamaño {1:2d}'.format(len(x),len(x[0])))\n",
    "print('{0:2d} datos de salida con tamaño {1:2d}'.format(len(y),len(y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamaño características de entrenamiento: 10000\n",
      "tamaño características de validacion: 11873\n",
      "--------------------------------------------------------------------\n",
      "tamaño groundtruth de entrenamiento: 10000\n",
      "tamaño groundtruth de validacion: 11873\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cf4c70bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train= x[:10000]\n",
    "y_train= y[:10000]\n",
    "t_train=time[:10000]\n",
    "\n",
    "\n",
    "\n",
    "y_plot=[]\n",
    "for i in range(0,len(y_train)):\n",
    "    y_plot.append(y_train[i][0])\n",
    "\n",
    "plt.plot(t_train,y_plot)\n",
    "    \n",
    "x_val = x[35000:len(x)-1]\n",
    "y_val = y[35000:]\n",
    "t_val=time[35000:len(x)-1]\n",
    "print('tamaño características de entrenamiento: {0:2d}'.format(len(x_train)))\n",
    "print('tamaño características de validacion: {0:2d}'.format(len(y_val)))\n",
    "print('--------------------------------------------------------------------')\n",
    "print('tamaño groundtruth de entrenamiento: {0:2d}'.format(len(x_train)))\n",
    "print('tamaño groundtruth de validacion: {0:2d}'.format(len(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "std_scale=skl.preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = std_scale.transform(x_train)\n",
    "x_val = std_scale.transform(x_val)\n",
    "std_scale=skl.preprocessing.StandardScaler().fit(y)\n",
    "y_train = std_scale.transform(y_train)\n",
    "y_val = std_scale.transform(y_val)\n",
    "#np.interp(a, (a.min(), a.max()), (-1, +1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZ-ve5xhXhMH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giopa\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# set random seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "input= torch.from_numpy(np.array(x_train))\n",
    "target = torch.from_numpy(np.array(y_train))\n",
    "#target = target.unsqueeze(0)\n",
    "test_input = torch.from_numpy(np.array(x_val))\n",
    "test_target = torch.from_numpy(np.array(y_val))\n",
    "# build the model\n",
    "sm = SMnet()\n",
    "sm.double()\n",
    "criterion = nn.L1Loss(size_average=False)\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(sm.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "TuzWqMWkkiXj",
    "outputId": "fb189ca9-e8be-485a-f2ab-b0e9bed07728",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 15.450243839336473   (0.0%)\n",
      "loss: 15.331489815593942   (0.2%)\n",
      "loss: 15.105778062695503   (0.4%)\n",
      "loss: 14.78357317011622   (0.6%)\n",
      "loss: 14.374276236674966   (0.8%)\n",
      "loss: 13.8860937338757   (1.0%)\n",
      "loss: 13.323854898408753   (1.2%)\n",
      "loss: 12.697484539700373   (1.4%)\n",
      "loss: 12.009627454543152   (1.6%)\n",
      "loss: 11.263505995635501   (1.8%)\n",
      "loss: 10.46122429776189   (2.0%)\n",
      "loss: 9.603469471545978   (2.2%)\n",
      "loss: 8.689470120129524   (2.4%)\n",
      "loss: 7.717535631392428   (2.6%)\n",
      "loss: 6.684345287371707   (2.8%)\n",
      "loss: 5.5854380690330325   (3.0%)\n",
      "loss: 4.41514495213098   (3.2%)\n",
      "loss: 3.166284721779133   (3.4%)\n",
      "loss: 1.830089447611899   (3.6%)\n",
      "loss: 0.4894557897476963   (3.8%)\n",
      "loss: 1.058725235794167   (4.0%)\n",
      "loss: 2.1728083883442046   (4.2%)\n",
      "loss: 2.952489469557986   (4.4%)\n",
      "loss: 3.4093660662398637   (4.6%)\n",
      "loss: 3.5707115663838684   (4.8%)\n",
      "loss: 3.465123812281677   (5.0%)\n",
      "loss: 3.1263980081628673   (5.2%)\n",
      "loss: 2.590036237648277   (5.4%)\n",
      "loss: 1.8922951503778893   (5.6%)\n",
      "loss: 1.0685177188870687   (5.8%)\n",
      "loss: 0.42742944825059637   (6.0%)\n",
      "loss: 0.6440315762680084   (6.2%)\n",
      "loss: 1.1760310439915151   (6.4%)\n",
      "loss: 1.4725553554811377   (6.6%)\n",
      "loss: 1.5681450660476999   (6.8%)\n",
      "loss: 1.4865226625029067   (7.0%)\n",
      "loss: 1.2451186663914773   (7.2%)\n",
      "loss: 0.8543463596380589   (7.4%)\n",
      "loss: 0.5192031329970217   (7.6%)\n",
      "loss: 0.3729299587858521   (7.8%)\n",
      "loss: 0.6811869660059207   (8.0%)\n",
      "loss: 0.8956824170663882   (8.2%)\n",
      "loss: 0.9023364162190588   (8.4%)\n",
      "loss: 0.719592647764536   (8.6%)\n",
      "loss: 0.41269798046601847   (8.8%)\n",
      "loss: 0.2625109561227602   (9.0%)\n",
      "loss: 0.4816075912298562   (9.2%)\n",
      "loss: 0.665594778152259   (9.4%)\n",
      "loss: 0.6630022320869262   (9.6%)\n",
      "loss: 0.49455698447575114   (9.8%)\n",
      "loss: 0.203210607254388   (10.0%)\n",
      "loss: 0.2490997457739782   (10.2%)\n",
      "loss: 0.41496040421838165   (10.4%)\n",
      "loss: 0.4416862573525997   (10.6%)\n",
      "loss: 0.33071012709012404   (10.8%)\n",
      "loss: 0.17882473450918346   (11.0%)\n",
      "loss: 0.22883793029652189   (11.2%)\n",
      "loss: 0.32833582352843804   (11.4%)\n",
      "loss: 0.25378723285964155   (11.6%)\n",
      "loss: 0.15371754239204394   (11.8%)\n",
      "loss: 0.22598324191587504   (12.0%)\n",
      "loss: 0.29934040680956076   (12.2%)\n",
      "loss: 0.273855357017329   (12.4%)\n",
      "loss: 0.17731194168324305   (12.6%)\n",
      "loss: 0.17750034906698908   (12.8%)\n",
      "loss: 0.2576843481160661   (13.0%)\n",
      "loss: 0.22635485443999082   (13.2%)\n",
      "loss: 0.24054188145027   (13.4%)\n",
      "loss: 0.19322265091025215   (13.6%)\n",
      "loss: 0.1886904741802431   (13.8%)\n",
      "loss: 0.21158583660819397   (14.0%)\n",
      "loss: 0.26880036842512   (14.2%)\n",
      "loss: 0.22298280487687572   (14.4%)\n",
      "loss: 0.10857987035383743   (14.6%)\n",
      "loss: 0.17691927371041827   (14.8%)\n",
      "loss: 0.20990233073100928   (15.0%)\n",
      "loss: 0.1554823037182842   (15.2%)\n",
      "loss: 0.1019968854783565   (15.4%)\n",
      "loss: 0.13261983992565074   (15.6%)\n",
      "loss: 0.12296090733383713   (15.8%)\n",
      "loss: 0.12039648984769735   (16.0%)\n",
      "loss: 0.08776173127413323   (16.2%)\n",
      "loss: 0.12304928703609952   (16.4%)\n",
      "loss: 0.1128649683558196   (16.6%)\n",
      "loss: 0.08335137282051852   (16.8%)\n",
      "loss: 0.107475469619991   (17.0%)\n",
      "loss: 0.09969743950523713   (17.2%)\n",
      "loss: 0.07332462308567522   (17.4%)\n",
      "loss: 0.09214437612659476   (17.6%)\n",
      "loss: 0.08300605766161018   (17.8%)\n",
      "loss: 0.07029810339058973   (18.0%)\n",
      "loss: 0.066327617385884   (18.2%)\n",
      "loss: 0.06332623426860162   (18.4%)\n",
      "loss: 0.06644507233516073   (18.6%)\n",
      "loss: 0.062380060235064816   (18.8%)\n",
      "loss: 0.054807350489051276   (19.0%)\n",
      "loss: 0.0721011123189742   (19.2%)\n",
      "loss: 0.04329515288375707   (19.4%)\n",
      "loss: 0.06764944818325735   (19.6%)\n",
      "loss: 0.06408985539983869   (19.8%)\n",
      "loss: 0.08457662498817253   (20.0%)\n",
      "loss: 0.09049564543020017   (20.2%)\n",
      "loss: 0.04577194910146243   (20.4%)\n",
      "loss: 0.12081528908090278   (20.6%)\n",
      "loss: 0.11072502385988625   (20.8%)\n",
      "loss: 0.1258322570016206   (21.0%)\n",
      "loss: 0.11220996997307564   (21.2%)\n",
      "loss: 0.12222503899239845   (21.4%)\n",
      "loss: 0.0947652253724437   (21.6%)\n",
      "loss: 0.15918765449392547   (21.8%)\n",
      "loss: 0.1288231404087251   (22.0%)\n",
      "loss: 0.0979782021052672   (22.2%)\n",
      "loss: 0.19022631052748284   (22.4%)\n",
      "loss: 0.15244552001915768   (22.6%)\n",
      "loss: 0.13599832343734286   (22.8%)\n",
      "loss: 0.16068358453363918   (23.0%)\n",
      "loss: 0.11923317678724332   (23.2%)\n",
      "loss: 0.05290660222968413   (23.4%)\n",
      "loss: 0.10158026832224887   (23.6%)\n",
      "loss: 0.14649005006319116   (23.8%)\n",
      "loss: 0.11558769128622837   (24.0%)\n",
      "loss: 0.058645971294617016   (24.2%)\n",
      "loss: 0.10391998436892591   (24.4%)\n",
      "loss: 0.1214554353556585   (24.6%)\n",
      "loss: 0.1462038010149882   (24.8%)\n",
      "loss: 0.18749909337868798   (25.0%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b52ca6f24d57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss: {}   ({}%)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Does the update\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# in your training loop:\n",
    "batch_size=5\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    batch = input[i:i+batch_size,:]\n",
    "    batch = batch.unsqueeze(0)\n",
    "    #print(batch.size())\n",
    "    batch_target = target[i:i+batch_size,:]\n",
    "    batch_target = batch_target.unsqueeze(0)\n",
    "    output = sm(batch)\n",
    "    loss = criterion(output, batch_target)\n",
    "    print(\"loss: {}   ({}%)\".format(loss,(i/5)))\n",
    "    loss.backward()\n",
    "    optimizer.step() # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = sm(input.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=Z.view(-1,3)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_pred,x_true=[],[]\n",
    "    y_pred,y_true=[],[]\n",
    "    z_pred,z_true=[],[]\n",
    "    for k in range(0,len(y_val)):\n",
    "        x_pred.append(Z[k][0].item())\n",
    "        x_true.append(y_train[k][0].item())\n",
    "        y_pred.append(Z[k][1])\n",
    "        y_true.append(y_train[k][1])\n",
    "        z_pred.append(Z[k][2])\n",
    "        z_true.append(y_train[k][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(mean_squared_error(x_true, x_pred))\n",
    "print('x RMSE: {}'.format(rms)) # Sin normalizar: 2.01914| Con normalizar: 2.0322\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_true, y_pred))\n",
    "print('y RMSE: {}'.format(rms)) # Sin normalizar: 0.6174| Con normalizar: 0.6184\n",
    "\n",
    "rms = sqrt(mean_squared_error(z_true, z_pred))\n",
    "print('z RMSE: {}'.format(rms)) # Sin normalizar: 0.1204| Con normalizar: 0.1091\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('Partida ajustada manual')\n",
    "\n",
    "x_p,y_p,z_p=[],[],[]\n",
    "\n",
    "for i in range(0,len(x_pred)):\n",
    "    x_p.append(x_pred[i])\n",
    "    y_p.append(y_pred[i])\n",
    "    z_p.append(z_pred[i])\n",
    "\n",
    "rms = sqrt(mean_squared_error(x_true, x_p))\n",
    "print('x RMSE: {}'.format(rms)) # Sin normalizar: 2.01914| Con normalizar: 2.0322\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_true, y_p))\n",
    "print('y RMSE: {}'.format(rms)) # Sin normalizar: 2.01914| Con normalizar: 2.0322\n",
    "\n",
    "rms = sqrt(mean_squared_error(z_true, z_p))\n",
    "print('z RMSE: {}'.format(rms)) # Sin normalizar: 2.01914| Con normalizar: 2.0322\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "#plt.plot(t_val,x_pred,'b')\n",
    "plt.plot(t_val,x_true,'r')\n",
    "#plt.plot(t_val,x_p,'g')\n",
    "plt.title('X[t]')\n",
    "plt.legend(['prediction','true','manual'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "#plt.plot(t_val,y_pred,'b')\n",
    "plt.plot(t_val,y_true,'r')\n",
    "#plt.plot(t_val,y_p,'g')\n",
    "plt.title('Y[t]')\n",
    "plt.legend(['prediction','true','manual'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "#plt.plot(t_val,z_pred,'b')\n",
    "plt.plot(t_val,z_true,'r')\n",
    "#plt.plot(t_val,z_p,'g')\n",
    "plt.title('Z[t]')\n",
    "plt.legend(['prediction','true','manual'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SM_LCTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
